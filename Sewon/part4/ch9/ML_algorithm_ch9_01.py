#CH09_01. Dimensionality Reduction
"""
1. 차원의 정의
차원
-공간 내 데이터의 위치를 나타내기 위해 필요한 축의 개수

차원과 변수
-데이터가 n개의 설명번수를 가진다면 n차원의 좌표상에서 표현할 수 있다.
-키와 몸무게, 머리 길이를 설명 번수로 갖는 데이터 -> 3차원

2. 차원의 저주
차원의 저주 (Curse of Dimensionality): 변수가 늘어나면서 차원이 커짐에 따라 발생하는 문제
-필요한 데이터 수의 지수 함수적 증가로 인한 정보의 밀도 감소
-공간을 설명하기 위한 데이터의 부족 -> 과적합 문제 & 성능 감소
 ex) 각 변수당 가질 수 있는 값이 4개일 때 -> 1차원: 4개, 2차원: 16개, 3차원: 64개, ... , N차원: 4^N개
      -> 변수가 증가 -> 공간을 설명하기 위한 데이터가 부족 -> 모델의 성능 저하

3. 차원 축소
착안점
-데이터를 잘 설명할 수 있는 변수의 개수 (Latent Space)는 현재 변수의 개수 (Observation Space)보다 작을 수 있다.
-데이터를 기반으로 잠재 공간을 파악하는 것

차원 축소 (Dimensionality Reduction)
1) 차원의 저주 해결
2) 연산량 감소
3) 시각화 용이

차원 축소 방법
1) 변수 선택 (Feature Selection)
-원본 데이터의 변수 중 불필요한 변수를 제거하는 방법
 ex) 몸무게, 키, 머리 길이 (3차원) -> 몸무게, 키 (2차원)

2) 변수 추출 (Feature Extraction)
-원본 데이터의 변수들을 조합해 새로운 변수를 생성하는 방법
 ex) 키, 몸무게, 머리 길이 (3차원) -> 체구(몸무게, 키), 머리 길이 (2차원)
      체구 = 0.3*몸무게 + 0.7*키

변수 추출 방법
① PCA (Principal Component Analysis)
② LDA (Linear Discriminant Analysis)
③ s-SNE (t-distributed Stochastic Neighbor Embedding)

Principal Component Analysis
-여러 변수의 정보를 담고 있는 주성분(Principal Component)이라는 새로운 변수를 생성하는 차원 축소 기법
-단순히 차원을 줄이기 보다는 관측된 차원이 아닌 실제 데이터를 설명하는 차원을 찾아서 데이터를 더 잘 이해하고자 함

 ex) 스프링 운동은 변수 하나로 설명 가능하다.
     -> 이 데이터를 만드는 근본 변수는 하나
     ex) 3대의 카메라: 3차원, 10대의 카메라: 10차원

PCA -> 분산(Variance)을 최대로 보존하는 초평면(Hyperplane) 선택
 *초평면: 선형 공간의 차원보다 한 단계 낮은 차원을 가진 부분 선형 공간을 평행 이동하여서 얻은 평면
-원본 데이터 셋과 투영된 초평면의 평균제곱거리를 최소화
-정보를 가장 적게 손실하기 때문 
-분산이 큰 초평면을 선택하는 이유? -> 차원이 줄어들어도 분산이 커서 각각의 데이터가 구별이 가능하므로

주성분
-첫 번째 주성분: 분산을 가장 크게 하는 축이 첫 번째 주성분
-두 번째 주성분: 첫 번째 주성분에 직교하면서 남은 분산을 최대로 보존하는 두 번째 축

주성분 축을 찾는 방법
Step1) 데이터 표준화: 평균을 0, 분산과 표준편차를 1로 만들어 데이터를 변환하는 방법
 cf. 정규화: 변수의 최소값은 0, 최대값은 1을 기준으로 0~1 사이의 값으로 변환하는 것 (=최소-최대 정규화)
-데이터를 표준화하지 않으면 값의 크기에 따라 공분산이 영향을 받음
 *공분산(Covariance): 두 변수의 관계를 나타내는 양

Step2) 공분산 행렬 생성

Step3) 고유값 분해 (Eigen Decomposition)
 ① 고유값(eigenvalue): Av=λv을 만족하는실수 λ
 ② 고유벡터(eigenvector): Av=λv을 만족하는 영벡터가 아닌 벡터 v 
 Av=λv -> Av-λv=(A-λ)v=0

Step4) K개 벡터의 새로운 Basis
K를 선택하는 방법
 1) Scree Plot -> 고유값이 급격이 작아지는 Elbow Point를 찾는다.
 2) Explained Variance

4. PCA
PCA 장점
-변수간 상관관계 및 연관성을 이용해 변수를 생성한다.
-차원 축소로 차원의 저주를 해결할 수 있다.

PCA 단점
-데이터에 선형성이 없다면 적용할 수 없다.
-데이터의 클래스를 고려하지 않기 때문에 최대 분산 방향이 특징 구분을 좋게 한다고 보장할 수 없다.
-주성분의 해석을 위한 도메인 지식이 필요하다.
"""