#CH04_01. Decision Tree

"""
1. 의사결정나무의 정의
-의사 결정 규칙과 그 결과물들을 트리 구조로 도식화한 것
-의사결정나무의 요소
 1) 뿌리 마디(Root Node): 나무 구조가 시작되는 마디, 전체 데이터로 이루어짐
 2) 중간 마디(Internal Node): 나무 구조의 중간에 있는 마디들
 3) 끝 마디(Terminal Node): 나무 구조의 끝에 위치하는 마디들, 더 이상 노드 생성X
 4) 깊이(Depth): 뿌리 노드에서 끝 마디까지의 노드 개수

2. 의사결정나무 알고리즘
-CART(Classification And Regression Tree) 알고리즘
 : 목적-각 분할에서 정보 이득을 최대화하는 것
   정보 이득의 최대화 -> 불순도의 감소

-분할: 노드에서 데이터가 어떤 기준에 의해 나눠지는 과정
-정보 이득(=불순도)
 1) 정답이 범주형 변수인 경우: 엔트로피 지수, 지니 지수
  -엔트로피(Entropy)
 정보 이론에서 정보의 불확실함의 정도를 나타내는 양
 데이터를 잘 구분할 수 없을수록 엔트로피는 커진다.
 정보 이득의 최대화 -> 불순도의 감소 -> 엔트로피의 감소

  -엔트로피 지수(Entropy Index) 수식
 entropy(A)= -∑ p log2(p): 확률과 로그를 곱하여 더한 식
 entropy(A)= ∑ R(-∑ p log2(p)): 분할된 데이터의 비율을 곱해서 더한 식

  -지니 지수(Gini Index)
 불평등의 정도를 나타내는 통계학적 지수
 데이터가 비슷하게 있을 수록 지니 지수는 높아짐
 정보 이득의 최대화 -> 불순도의 감소 -> 지니 지수 감소

  -지니 지수 수식
 I(A)= 1-∑(p)^2: 1에서 속할 확률들의 제곱을 더한 것을 뺀 값
 I(A)= ∑ R(1-∑(p)^2): 분할된 데이터의 비율을 곱해서 더한 식

  *엔트로피 지수가 지니 지수보다 조금 더 높게 계산되는 것을 볼 수 있음

 2) 정답이 연속병 변수인 경우: MSE를 이용한 분산량 감소
  -범주형 변수:
   데이터의 특성의 유무로 분할을 한다.
   정보 이득을 최대화하는 분할을 선택한다.
  -연속형 변수:
   특성의 유무로 나눌 수 없다.
   경계값을 찾고 경계값과의 비교를 통해 데이터를 분할한다.
    *연속형 변수의 경계값을 찾는 방법
    1) 변수 값에 따라 데이터를 정렬한다.
    2) 정답이 바뀌는 경계 지점을 찾는다.
    3) 경계의 평균값을 기준을 잡는다.
    4) 구간별 경계값을 기준으로 불순도를 계산한다.
    5) 가장 불순도를 낮추는 구간을 경계로 나눈다.

3. 변수 중요도(Feature Importance)
-의사결정나무에서 어떤 변수가 가장 중요한지를 나타내는 정도
-불순도를 가장 크게 감소시키는 변수의 중요도가 가장 크다
 (분순도의 감소는 정보 이득의 최대화와 동일한 뜻)

-지니 지수를 이용한 변수 중요도 수식
 I(C)= W G(C) - W left G (C left) - W right G (C right)
 W: 부모 노드의 데이터 양
 G(C): 지니 지수
 부모 노드의 지시 지수에서 자식 노드의 지니 지수를 뺀다.
 가중치(W)는 전체 데이터 대비 노드에 있는 데이터 수의 비율

4. 가지치기
-Full Tree
 모든 끝 마디에서의 순도가 100%인 상태
 분기가 너무 많아서 과대 적합(Overfitting) 위험이 발생할 수 있음
-가지치기: 분기가 너무 많아지는 것을 막기 위해서 적절한 수준에서 끝 노드를 결합해주는 것
 1) 사전 가지치기(Pre-Pruning)
  : 의사결정나무의 최대 Depth나 노드의 최소 개수를 미리 지정해 더 이상의 분할 방지
 2) 사후 가지치기(Post-Pruning)
  : 의사결정나무를 만든 후 데이터가 적은 노드를 삭제 or 병합

*의사결정나무의 장단점
장점
-모델의 예측 결과를 해석하고 이해하기 쉽다. (if ... then ...)
-데이터를 가공할 필요가 거의 없다.

단점
-연속형 변수를 범주형 값으로 취급하기 때문에 분리의 경계점 부근에서 예측 오류가 클 수 있다.
-노이즈 데이터에 영향을 크게 받는다.
 -> Overfitting 문제가 발생하기 쉽다.
"""