{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNpF1f4+GI3He01gKwPU9tT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Bagging\\\n","Bootstrap + Aggregation\n","\n","Bootstrap이란?\n","- Train Data에서 여러번 복원 추출하는 Random Sampling 기법\n","- 추출된 샘픔들을 부트스트랩 샘플이라고 부른다\n","- 이론적으로 36.8%의 샘플이 뽑히지 않게 됨(Out of Bag샘플)\n","\n","> OOB(Out of Bag) 평가\\\n","추출되지 않는 샘플을 이용해 Cross Validation(교차 검증) 에서 Valid 데이터로 사용할 수 있음\n"],"metadata":{"id":"AdwIip5afs3S"}},{"cell_type":"markdown","source":["Aggregation이란?\n","\n","생성된 약 분류기들의 예측 결과를 Voting을 통해 결합\n","\n","- **Voting 2가지 방법**\n","1. Hard Voting\\\n","예측한 결과값 중 다수의 분류기가 결정한 값을 최종 예측값으로 선정\n","\n","2. Soft Voting\\\n","분류기가 예측한 확률값의 평균으로 결정\n","0과 1의 각 예측한 확률값 평균에서 둘중 높은 확률값을 채택"],"metadata":{"id":"7Zj1ohYwjKde"}},{"cell_type":"markdown","source":["Bagging 장점\n","- 분산을 줄이는 효과가 있음\n"," - 원래 추정 모델이 불안정하면 분산 감소 효과를 얻을 수 있음\n"," - 과대 적합이 심한 모델에 적합"],"metadata":{"id":"JYpE6GIqkdCg"}},{"cell_type":"markdown","source":["Random Forest 정의\n","- Decision Tree + Bagging(여러개의 나무들을 모아서 숲을 만든다)\n","- 분산이 큰 Decision Tree + 분산을 줄일 수 있는 Bagging\n","\n","> Random Forest와 무작위성\n","- 무작위성을 더 강조하여서 의사결정나무들이 서로 조금씩 다른 특성을 갖음\n"," - 변수가 20개가 있다면 5개의 변수만 선택해서 의사결정나무를 생성\n","- 의사결정나무의 예측들이 비상관화되어 일반화 성능을 향상 시킴(Overfitting 가능성 줄어들음)\n","\n","Random Forest 학습방법\n","1. Bootrap 방법으로 T개의 부트스트랩 샘플을 생성\n","2. T개의 의사결정나무들을 만든다.\n","3. 의사결정나무 분류기들을 하나의 분류기로 결합함\n"],"metadata":{"id":"OGfBr1VZlB2N"}}]}